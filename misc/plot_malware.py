"""

Read training history files, compute metrics, and plot stats comparing various sets of histories.
Poorly designed but I was low on time.

"""

import matplotlib.pyplot as plt
import os
import sys
from os import listdir, mkdir
from os.path import isfile, join, isdir
import numpy as np
import itertools
import matplotlib.ticker as ticker
from matplotlib.colors import LinearSegmentedColormap
from sklearn import svm
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score, average_precision_score
from sklearn.utils import shuffle
import pickle
from scipy.stats import pearsonr, spearmanr
from save import save_csv
from plot import History, init_dir



# Results for a single repetition of an experiment
class Run():
    def __init__(self):
        self.family_train_histories = {} #dict of family:history?
        self.test_scores = {} #iter : scoring
        self.aucs = {} #iter : {family:auc}
        self.bal_accs = {} #iter : acc


# Encapsulates classification scores, with one score per family for each sample
class Scores():
    def __init__(self, scores, labels, families):
        self.scores = scores
        self.labels = labels
        self.families = families
        
    #rbf?
    def compute_svm_metric(self, metric='balanced_accuracy', kernel='linear', C=1, folds=1): 
        X = self.scores
        y = self.labels

        kfoldcv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)
        model = make_pipeline(StandardScaler(), svm.SVC(kernel=kernel, C=C, class_weight='balanced'))

        scores = cross_val_score(model, X, y, scoring=metric, cv=kfoldcv, n_jobs=-1)

        mean_metric = sum(scores) / len(scores)
        return mean_metric


    def compute_roc_auc(self, family):
        try:
            index = self.families.index(family)
        except ValueError as e:
            print("Could not find family " + family + " in data")
            return

        scores = self.scores[:,index]
        
        fpr, tpr, thresholds = roc_curve(self.labels, scores, pos_label=family)
        roc_auc = auc(fpr, tpr)
        
        return fpr, tpr, thresholds, roc_auc



def load_score_file(fname):
    scores = np.loadtxt(fname=fname, delimiter=',', skiprows=1)
    
    classes, scores = np.hsplit(scores, [1]) #split first label column into its own array
    
    labels = None
    with open(fname, 'r') as file:
        header = file.readline().strip()
        labels = header.split(',')[1:]
        if len(labels) < 1:
            labels = None

    # Hacky method of making this labeled.  This function should really be rewritten, along with how the values are written
    label_map = {i:labels[i] for i in range(len(labels))}
    labeled_classes = [label_map[classes[i][0]] for i in range(len(classes))]
    classes = np.array(labeled_classes)
    
    return scores, classes, labels



# Should probably be partially moved into Run constructor
def load_runs(num_runs, data_dir, families, score_iters, fname_template, momentum, restarts):
    runs = []
    for r in range(num_runs):
        print(r)
        run = Run()
    
        # training history for each family
        for family in families: 
            h = History(description=family + " m=" + str(momentum))
            f = fname_template.replace('{:s}', family, 1).replace('{:.6f}','{:.6f}'.format(float(momentum))).replace('{:d}', str(r),1)
            h.load(data_dir, f, range(0, restarts))
            run.family_train_histories[family] = h
            
        # test scores for each iter
        for itr in score_iters:
            scores, labels, fams = load_score_file(data_dir + str(itr) + ' iters/' + '_scores_' + str(r) + '.csv')
            run.test_scores[itr] = Scores(scores, labels, fams)
            
        #accuracies
        for itr in score_iters:
            acc = run.test_scores[itr].compute_svm_metric(metric='balanced_accuracy', kernel='rbf', C=10, folds=5)
            #acc = run.test_scores[itr].compute_svm_metric(metric='balanced_accuracy', kernel='linear', C=10, folds=5)
            run.bal_accs[itr] = acc
            print(acc)
            
        for itr in score_iters:
            run.aucs[itr] = {}
            for family in families:
                fpr, tpr, thresholds, auc = run.test_scores[itr].compute_roc_auc(family)
                run.aucs[itr][family] = auc
                
        for itr in score_iters:
            run.test_scores[itr] = None
        
        runs.append(run)
            
    return runs
    
    
    
def plot_auc_all(runs, score_iters, num_runs, families, outfile=None, show=False):
    fig, ax = plt.subplots()#figsize=(10,6))
    
    saves = [score_iters]
    headers = ['Iterations']
    
    for family in families:
        family_aucs = []
        for itr in score_iters:
            auc = sum((run.aucs[itr][family] for run in runs)) / num_runs
            family_aucs.append(auc)
        plt.plot(score_iters, family_aucs, label=family)
        saves.append(family_aucs)
        headers.append(family)

    #plt.plot([0,score_iters[-1]], [0,0], linestyle='--', color='black')
    #plt.xlim(score_iters[0], score_iters[-1])
    ax.set_xlim(right=score_iters[-1])
    #ax.hlines(y=0, xmin=0, xmax=score_iters[-1], linewidth=1, color='black')
    
    plt.xlabel("Iterations")
    plt.ylabel("AUC")
    #ax.legend(loc='upper right', fontsize='small')
    plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)
    plt.axhline(y=0, linestyle='--', color='black')
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    #plt.title("")
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()

    save_csv(saves, outfile[:-4] + '.csv', headers=headers) 
    
    

def plot_delta_auc_total(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):
    aucs = []
    for itr in score_iters:
        mean_auc0 = 0
        mean_aucM = 0
        for family in families:
            auc0 = sum((run.aucs[itr][family] for run in runs0)) / num_runs
            aucM = sum((run.aucs[itr][family] for run in runsM)) / num_runs
            mean_auc0 += auc0
            mean_aucM += aucM
        mean_auc0 /= len(families)
        mean_aucM /= len(families)
        aucs.append(mean_aucM - mean_auc0)

    fig, ax = plt.subplots()#figsize=(10,6))
    plt.plot(score_iters, aucs)
    
    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ AUC")
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    plt.axhline(y=0, linestyle='--', color='black')
    #plt.title("")
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()
    
    save_csv([score_iters, aucs], outfile[:-4] + '.csv', headers=['Iterations', 'Mean Change in AUC']) 
    
    
def plot_delta_auc_individual(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):
    aucs = {}
    for family in families:
        family_aucs = []
        for itr in score_iters:
            auc0 = sum((run.aucs[itr][family] for run in runs0)) / num_runs
            aucM = sum((run.aucs[itr][family] for run in runsM)) / num_runs
            family_aucs.append(aucM - auc0)
        aucs[family] = family_aucs

    fig, ax = plt.subplots()#figsize=(10,6))
    for family in families:
        plt.plot(score_iters, aucs[family], label=family)
    #plt.plot([0,score_iters[-1]], [0,0], linestyle='--', color='black')
    #plt.xlim(score_iters[0], score_iters[-1])
    ax.set_xlim(right=score_iters[-1])
    #ax.hlines(y=0, xmin=0, xmax=score_iters[-1], linewidth=1, color='black')
    
    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ AUC")
    #ax.legend(loc='upper right', fontsize='small')
    plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)
    plt.axhline(y=0, linestyle='--', color='black')
    #ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 5))
    #plt.title("")
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()
    
    saves = [score_iters]
    headers = ["Iterations"]
    for family in families:
        saves.append(aucs[family])
        headers.append(family)
    save_csv(saves, outfile[:-4] + '.csv', headers=headers) 
    
    
    
def plot_mean_auc(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):
    aucs0 = []
    aucsM = []
    for itr in score_iters:
        mean_auc0 = 0
        mean_aucM = 0
        for family in families:
            auc0 = sum((run.aucs[itr][family] for run in runs0)) / num_runs
            aucM = sum((run.aucs[itr][family] for run in runsM)) / num_runs
            mean_auc0 += auc0
            mean_aucM += aucM
        mean_auc0 /= len(families)
        mean_aucM /= len(families)
        aucs0.append(mean_auc0)
        aucsM.append(mean_aucM)

    fig, ax = plt.subplots()#figsize=(10,6))
    plt.plot(score_iters, aucs0, label='no momentum')
    plt.plot(score_iters, aucsM, label='nesterov=0.4')
    
    plt.xlabel("Iterations")
    plt.ylabel("AUC")
    ax.legend(loc='lower right')
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    #plt.title("")
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()
    
    save_csv([score_iters, aucs0, aucsM], outfile[:-4] + '.csv', headers=['Iterations', 'AUC0', 'AUCM']) 


def plot_mean_auc_individual(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):
    aucs0 = {}
    aucsM = {}
    for family in families:
        family_aucs0 = []
        family_aucsM = []
        for itr in score_iters:
            auc0 = sum((run.aucs[itr][family] for run in runs0)) / num_runs
            aucM = sum((run.aucs[itr][family] for run in runsM)) / num_runs
            family_aucs0.append(auc0)
            family_aucsM.append(aucM)
        aucs0[family] = family_aucs0
        aucsM[family] = family_aucsM

    fig, ax = plt.subplots()#figsize=(10,6))
    for family in families:
        plt.plot(score_iters, aucs0[family], label=family + ' m=0')
        plt.plot(score_iters, aucsM[family], label=family + ' m=0.4')
    #plt.plot([0,score_iters[-1]], [0,0], linestyle='--', color='black')
    #plt.xlim(score_iters[0], score_iters[-1])
    ax.set_xlim(right=score_iters[-1])
    ax.ticklabel_format(useOffset=False, style='plain', axis='y')
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    
    plt.xlabel("Iterations")
    plt.ylabel("AUC")
    ax.legend(loc='lower right')
    #plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)
    #plt.title("")
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()   

    saves = [score_iters]
    headers = ["Iterations"]
    for family in families:
        saves.append(aucs0[family])
        saves.append(aucsM[family])
        headers.append(family + ' AUC0')
        headers.append(family + ' AUCM')
    save_csv(saves, outfile[:-4] + '.csv', headers=headers)     
    
    
    
def plot_mean_svm_accuracy(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):
    accs0 = []
    accsM = []
    for itr in score_iters:
        acc0 = sum((run.bal_accs[itr] for run in runs0)) / num_runs * 100
        accM = sum((run.bal_accs[itr] for run in runsM)) / num_runs * 100
        accs0.append(acc0)
        accsM.append(accM)
        
    fig, ax = plt.subplots()
    plt.plot(score_iters, accs0, label='no momentum')
    plt.plot(score_iters, accsM, label='nesterov=0.4')
    
    #ax.set_xlim(right=score_iters[-1])
    plt.xlabel("Iterations")
    plt.ylabel("Balanced Accuracy")
    ax.legend(loc='lower right')    
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
        
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()    
    
    save_csv([score_iters, accs0, accsM], outfile[:-4] + '.csv', headers=['Iterations', 'Accuracy0', 'AccuracyM']) 
    
def plot_delta_mean_svm_accuracy(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):
    accs = []
    for itr in score_iters:
        acc0 = sum((run.bal_accs[itr] for run in runs0)) / num_runs * 100
        accM = sum((run.bal_accs[itr] for run in runsM)) / num_runs * 100
        accs.append(accM - acc0)
    
    fig, ax = plt.subplots()
    plt.plot(score_iters, accs)
    
    #ax.set_xlim(right=score_iters[-1])
    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ Balanced Accuracy %")    
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    plt.axhline(y=0, linestyle='--', color='black')
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()    
    
    save_csv([score_iters, accs], outfile[:-4] + '.csv', headers=['Iterations', 'Mean Change in Accuracy']) 
    
# Assumes 1 restart
def plot_family_mean_score(runs0, runsM, iter_range, num_runs, families, outfile=None, show=False):
    saves = [list(range(iter_range[1]))]
    headers = ["Iterations"]

    fig, ax = plt.subplots()
    for family in families:
        h0 = sum((run.family_train_histories[family].histories[0][iter_range[0]:iter_range[1]] for run in runs0)) / num_runs
        hM = sum((run.family_train_histories[family].histories[0][iter_range[0]:iter_range[1]] for run in runsM)) / num_runs
        plt.plot(list(range(h0.shape[0])), h0, label=family + ' no momentum')
        plt.plot(list(range(hM.shape[0])), hM, label=family + ' nesterov=0.4')
        
        saves.append(h0)
        saves.append(hM)
        headers.append(family + ' 0')
        headers.append(family + ' M')
        
    ax.legend(loc='lower right') 
    plt.xlabel("Iterations")
    plt.ylabel("Score")    
    plt.xlim(left=iter_range[0])
    plt.xlim(right=iter_range[1])
    
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close() 
    
    
    save_csv(saves, outfile[:-4] + '.csv', headers=headers)    


def plot_family_mean_score_diff(runs0, runsM, iter_range, num_runs, families, outfile=None, show=False):
    saves = [list(range(iter_range[1]))]
    headers = ["Iterations"]

    fig, ax = plt.subplots()
    for family in families:
        h0 = History(description=family + " m=" + str(0))
        hM = History(description=family + " m=" + str(0.4))
        h0.histories = np.zeros(runs0[0].family_train_histories[family].histories.shape)
        hM.histories = np.zeros(runsM[0].family_train_histories[family].histories.shape)
        for run in runs0:
            h0.histories += run.family_train_histories[family].histories
        for run in runsM:
            hM.histories += run.family_train_histories[family].histories
        h0.histories /= num_runs
        hM.histories /= num_runs
        #plt.plot(list(range(iter_range[0],iter_range[1])), h0.mean()[iter_range[0]:iter_range[1]], label=family + ' no momentum')
        #plt.plot(list(range(iter_range[0],iter_range[1])), hM.mean()[iter_range[0]:iter_range[1]], label=family + ' nesterov=0.4')
        plt.plot(list(range(iter_range[0],iter_range[1])), hM.mean_diff(h0)[iter_range[0]:iter_range[1]], label=family)
        
        saves.append(hM.mean_diff(h0)[iter_range[0]:iter_range[1]])
        headers.append(family + ' Score Change')
        
    plt.axhline(y=0, linestyle='--', color='black')
    ax.legend(loc='lower right') 
    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ Score")    
    plt.xlim(left=iter_range[0])
    plt.xlim(right=iter_range[1])
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close() 
    
    save_csv(saves, outfile[:-4] + '.csv', headers=headers)   
    
    
#assuming 1 restart
def plot_total_mean_score_diff(runs0, runsM, iter_range, num_runs, families, outfile=None, show=False):
    diff = np.zeros((iter_range[1]-iter_range[0]))
    for family in families:
        h0 = sum((run.family_train_histories[family].histories[0][iter_range[0]:iter_range[1]] for run in runs0)) / num_runs
        hM = sum((run.family_train_histories[family].histories[0][iter_range[0]:iter_range[1]] for run in runsM)) / num_runs
        diff += hM - h0
    diff /= len(families)
    
    fig, ax = plt.subplots()    
    plt.plot(list(range(iter_range[0],iter_range[1])), diff)
        
    plt.axhline(y=0, linestyle='--', color='black')
    #ax.legend(loc='lower right') 
    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ Score")    
    plt.xlim(left=iter_range[0])
    plt.xlim(right=iter_range[1])
    ax.set_xticks(np.arange(iter_range[0], iter_range[1]+1, 25))
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()   

    save_csv([list(range(iter_range[0],iter_range[1])), diff], outfile[:-4] + '.csv', headers=['Iterations', 'Mean Score Diff'])   


def plot_score_diff_stddev(runs0, runsM, iter_range, num_runs, families, outfile=None, show=False):
    fig, ax = plt.subplots()
    
    num_iters = iter_range[1] - iter_range[0]
    num_runs = len(runs0)
    
    all_diffs = np.zeros((0,num_iters))
    for family in families:
        diffs = np.zeros((num_runs,num_iters)) 
        for r in range(num_runs):
            run0 = runs0[r]
            runM = runsM[r]
            diffs[r] = runM.family_train_histories[family].histories - run0.family_train_histories[family].histories
            
        all_diffs = np.vstack((all_diffs, diffs))
        
    diffs = all_diffs
    mean_diffs = np.mean(diffs, axis=0)
    stddevs = np.std(diffs, axis=0)
        
    plt.errorbar(list(range(iter_range[0],iter_range[1])), mean_diffs, stddevs, color='tab:blue', alpha=0.2)#color = '#5555ff')#, marker='^') #label=family
    plt.plot(list(range(iter_range[0],iter_range[1])), mean_diffs, color='tab:blue')
        
    plt.axhline(y=0, linestyle='--', color='black')
    #ax.legend(loc='lower right') 
    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ Score")    
    plt.xlim(left=iter_range[0])
    plt.xlim(right=iter_range[1])
    ax.set_xticks(np.arange(iter_range[0], iter_range[1]+1, 25))

    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()   
    
    
    

#AUC change stddev?
#or AUC stddev by family?
#plot AUC mean,stddev EITHER with or without momentum, not both
def plot_single_auc_stddev(runs, score_iters, num_runs, families, outfile=None, show=False):
    fig, ax = plt.subplots()
    
    for family in families:
        aucs = np.zeros( (num_runs, len(score_iters)) )
        
        for r in range(num_runs):
            for i in range(len(score_iters)):
                itr = score_iters[i]
                run = runs[r]
                auc = run.aucs[itr][family]
                aucs[r][i] = auc
                
        means = np.mean(aucs, axis=0)
        stddevs = np.std(aucs, axis=0, ddof=1)

        plt.errorbar(score_iters, means, stddevs, color='tab:blue', alpha=0.2)#color = '#5555ff')#, marker='^') #label=family
        plt.plot(score_iters, means, color='tab:blue')

    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ AUC")
    ax.set_xlim(right=score_iters[-1])
    ax.ticklabel_format(useOffset=False, style='plain', axis='y')
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()   


    
#plot mean,stddev of change in AUC due to momentum
def plot_mean_aucdiff_stddev(runs0, runsM, score_iters, num_runs, families, outfile=None, show=False):    
    fig, ax = plt.subplots()
    
    for family in families:
        aucs0 = np.zeros( (num_runs, len(score_iters)) )
        aucsM = np.zeros( (num_runs, len(score_iters)) )
        for r in range(num_runs):
            for i in range(len(score_iters)):
                itr = score_iters[i]
                run0 = runs0[r]
                runM = runsM[r]
                auc0 = run0.aucs[itr][family]
                aucs0[r][i] = auc0
                aucM = runM.aucs[itr][family]
                aucsM[r][i] = aucM
                
        diffs = aucsM - aucs0
        means = np.mean(diffs, axis=0)
        stddevs = np.std(diffs, axis=0, ddof=1)

        #plt.errorbar(score_iters, means, stddevs, color='black', alpha=0.5)#color = '#5555ff')#, marker='^') #label=family
        #plt.plot(score_iters, means, color='tab:blue')
        
        plt.errorbar(score_iters, means, stddevs, color='black', alpha=0.6)#color = '#5555ff')#, marker='^') #label=family
        plt.plot(score_iters, means, color='blue')
        

    plt.xlabel("Iterations")
    plt.ylabel("$\Delta$ AUC")
    ax.set_xlim(right=score_iters[-1])
    ax.set_ylim(bottom=-0.1, top=0.1)
    ax.ticklabel_format(useOffset=False, style='plain', axis='y')
    ax.set_xticks(np.arange(0, score_iters[-1]+1, 25))
    
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()   

   
    
    
    
    
    

def get_mean_history(runs, family):
    h = None
    for run in runs:
        x = run.family_train_histories[family].mean()
        if h is None:
            h = np.zeros((x.shape))
        h += x
    h /= len(runs)
    return h
    
    
def get_mean_score(history):
    pass
    

# Going to want to get mean AND stddev for each run in a family
def get_score_diff(hM, h0):
    pass
    



def compute_correlation(runs0, runsM, num_runs, score_iters, families, size=1, outfile=None, show=False):
    dscores = []
    dAUC = []
    #dscoresFam = {}
    #dAUCFam = {}
    #for family in families:
    #    dscoresFam[family] = []
    #    dAUCFam[family] = []
    for i in range(num_runs):
        run0 = runs0[i]
        runM = runsM[i]
        for family in families:
            for itr in score_iters:
                score0 = run0.family_train_histories[family].max()[itr-1]
                scoreM = runM.family_train_histories[family].max()[itr-1]
                dscore = scoreM - score0
                dauc = runM.aucs[itr][family] - run0.aucs[itr][family]
                #print((dscore, dauc))
                dscores.append(dscore)
                dAUC.append(dauc)
                #dscoresFam[family].append(dscore)
                #dAUCFam[family].append(dauc)
                
    plt.scatter(dscores, dAUC, s=size)
    #for family in families:
    #    plt.scatter(dscoresFam[family], dAUCFam[family], s=size)
    plt.xlabel("$\Delta$ Score")
    plt.ylabel("$\Delta$ AUC")  
    if outfile is not None:
        init_dir(outfile)
        plt.savefig(outfile, dpi=200, bbox_inches='tight')
    if show:
        plt.show()
    plt.close()    

    r,p = pearsonr(dscores, dAUC)
    #r,p = spearmanr(dscores, dAUC)
    
    if outfile is not None:
        save_csv([dscores, dAUC], outfile[:-4] + '.csv', headers=['Change in Scores', 'Change in AUC'])   
    
    return r, p
    
    




def plotMultiIters():
    N = 10
    M = 30
    T = 100000
    momentum = 0.4
    nesterov = False
    nesterov = True
    smoothing = 0.001
    start_iter = 0
    end_iter = 300
    score_iters = [5,10,15,20,25,35,50,100,200,300]
    first_restart = 0
    last_restart = 1
    num_runs = 100
    show = False
    folds = 5
    #kernel = 'rbf' #'linear'
    #C = 10
    metric = 'balanced_accuracy'#'f1_macro'
    #families = ['winwebsec', 'zbot', 'zeroaccess']
    families = ['VBInject','Winwebsec','Renos','OnLineGames','Startpage', 'VB', 'Vobfus', 'CeeInject', 'Lolyda.BF', 'Zbot', 'FakeRean', 'Agent', 'Wintrim.BX', 'Allaple.A', 'Cycbot.G']#,'Vundo','Toga!rfn','Rimecud.A']
    load_processed = True
    
    nest = 0
    if nesterov:
        nest = 1
    
    data_dir = 'output/malware/extended/test/multiIter/15 families/'
    if not os.path.isdir(data_dir):
        print("NO DATA FOUND")
        exit()
        
    output_dir = 'results/final/malware/plots/test/'  #data_dir + 'plots/'
    pkl_dir = 'results/final/malware/saved/plus5/'
    init_dir(output_dir)
    
    # Have to use str for floats since unknown precision
    dir0 = data_dir + "N={:d} M={:d} T={:d} m={:s} nesterov={:d} smooth={:s}/".format(N,M,T,str(0),nest,str(smoothing))
    dirM = data_dir + "N={:d} M={:d} T={:d} m={:s} nesterov={:d} smooth={:s}/".format(N,M,T,str(momentum),nest,str(smoothing)) 
    filename = "{:d}_{:s}_{:d}_hmmM_{:.6f}.csv"
    
    
    
    
    #Get mean AUC,Acc across all runs for each iter
    #Line plot diff in metric vs iters
    #Score plot for each family
    #AUC plot for each family
    #Accuracy/F1 plot for each family 
    
    if not load_processed:
        runs0 = load_runs(num_runs, dir0, families, score_iters, filename, 0, last_restart)
        runsM = load_runs(num_runs, dirM, families, score_iters, filename, momentum, last_restart)
    else:
        runs0 = pickle.load(open(pkl_dir + 'runs0.pkl', "rb" ))
        runsM = pickle.load(open(pkl_dir + 'runsM.pkl', "rb" ))

    #At each scoring iteration
    #   Output the mean diff in score with momentum for each family over all runs
    #   Output the total mean diff in score with momentum over all runs
    #Comment out the AUC/SVM code to reduce runtime   


    # Histories diffs
    # Mean diff in score across all families
    # Mean diff in score for each family
    # At all iters?
    # At each iter that was scored
    print("Family Mean Diff")
    tot_mean_diff = None
    for family in families:
        mean_diff = None
        for i in range(num_runs):
            h0 = runs0[i].family_train_histories[family]
            hm = runsM[i].family_train_histories[family]
            if mean_diff is None:
                mean_diff = hm.mean_diff(h0)
            else:
                mean_diff += hm.mean_diff(h0)
        mean_diff /= num_runs
        #print(family + ": " + str(mean_diff[9]))
        if tot_mean_diff is None:
            tot_mean_diff = mean_diff
        else:
            tot_mean_diff += mean_diff
    tot_mean_diff /= len(families)    
    print()
    
    
    print("Family Mean AUC")
    for itr in score_iters:
        print(itr)
        mean_auc0 = 0
        mean_aucM = 0
        for family in families:
            auc0 = sum((run.aucs[itr][family] for run in runs0)) / num_runs
            aucM = sum((run.aucs[itr][family] for run in runsM)) / num_runs
            print(family + " 0: " + str(auc0))
            print(family + " M: " + str(aucM))
            mean_auc0 += auc0
            mean_aucM += aucM
        mean_auc0 /= len(families)
        mean_aucM /= len(families)
        print("Total Mean AUC 0: " + str(mean_auc0))
        print("Total Mean AUC M: " + str(mean_aucM))
        print()

    print("Balanced Accuracy")
    for itr in score_iters:
        print(itr)
        acc0 = sum((run.bal_accs[itr] for run in runs0)) / num_runs
        accM = sum((run.bal_accs[itr] for run in runsM)) / num_runs
        print("Mean Balanced Accuracy 0: " + str(acc0))
        print("Mean Balanced Accuracy M: " + str(accM))
    print()


    for i in range(end_iter):
        print(str(i) + ': ' + str(tot_mean_diff[i]))
        

    if not load_processed:
        with open(pkl_dir + 'runs0.pkl', 'wb') as outfile:
            pickle.dump(runs0, outfile, pickle.HIGHEST_PROTOCOL)
        with open(pkl_dir + 'runsM.pkl', 'wb') as outfile:
            pickle.dump(runsM, outfile, pickle.HIGHEST_PROTOCOL)
    
    
    #Plot AUC for all families
    plot_auc_all(runs0, score_iters, num_runs, families, outfile=output_dir+'auc_all0.png', show=False)
    plot_auc_all(runsM, score_iters, num_runs, families, outfile=output_dir+'auc_allM.png', show=False)
    
    # Plot change in AUC over iters for all families
    # Maybe add stddev?
    plot_delta_auc_total(runs0, runsM, score_iters, num_runs, families, outfile=output_dir+'delta_auc_all_families.png', show=False)
    
    # Plot change in AUC over iters for each family on 1 plot
    plot_delta_auc_individual(runs0, runsM, score_iters, num_runs, families, outfile=output_dir+'delta_auc_individual.png', show=False)
    plot_delta_auc_individual(runs0, runsM, score_iters[:7], num_runs, families, outfile=output_dir+'delta_auc_individual_50.png', show=False)
    
    # Plot AUC total
    plot_mean_auc(runs0, runsM, score_iters, num_runs, families, outfile=output_dir+'mean_auc_all.png', show=False)
    # Plot AUC for each
    for family in families:
        plot_mean_auc_individual(runs0, runsM, score_iters, num_runs, [family], outfile=output_dir+'AUC/auc_' + family + '.png', show=False)
        
    # Plot SVM accuracy
    plot_mean_svm_accuracy(runs0, runsM, score_iters, num_runs, families, outfile=output_dir+'svm_all.png', show=False)
    # Plot change in SVM accuracy
    plot_delta_mean_svm_accuracy(runs0, runsM, score_iters, num_runs, families, outfile=output_dir+'delta_svm_all.png', show=False)
    
    # Plot score history overall
    # Doesn't make a ton of sense to do this.  Better to do for each family
    
    # Plot score history for each family
    for family in families:
        plot_family_mean_score(runs0, runsM, [0, 300], num_runs, [family], outfile=output_dir+'scores300/score_' + family + '.png', show=False)
        plot_family_mean_score(runs0, runsM, [0, 50], num_runs, [family], outfile=output_dir+'scores50/score_' + family + '.png', show=False)
    
    # Plot difference in score history for each family
    for family in families:
        plot_family_mean_score_diff(runs0, runsM, [0, 300], num_runs, [family], outfile=output_dir+'scores300/diff_' + family + '.png', show=False)
    
    # Plot difference in score history for all families(with stddev?)
    plot_total_mean_score_diff(runs0, runsM, [0, 300], num_runs, families, outfile=output_dir+'mean_score_diff.png', show=False)
    
    # Plot with both change in AUC for all(or just 1) family AND change in score for comparison
    '''
        
    
    '''
    for family in families:
        plot_score_diff_stddev(runs0, runsM, [0, 300], num_runs, [family], outfile=output_dir+'stddevs/'+family+'.png', show=False)
    plot_score_diff_stddev(runs0, runsM, [0, 300], num_runs, families, outfile=output_dir+'score_stddev.png', show=False)



    #AUC itself
    for family in families:
        plot_single_auc_stddev(runs0, score_iters, num_runs, [family], outfile=output_dir+'auc_stddev/'+family+'_0.png', show=False)
        plot_single_auc_stddev(runsM, score_iters, num_runs, [family], outfile=output_dir+'auc_stddev/'+family+'_M.png', show=False)


    for family in families:
        plot_mean_aucdiff_stddev(runs0, runsM, score_iters, num_runs, [family], outfile=output_dir+'auc_diffs_stddev/'+family+'.png', show=False)



    '''
    Start run for 5 iters in separate directory
    '''
    
    for family in families:
        print(family)
        r,p = compute_correlation(runs0, runsM, num_runs, score_iters, [family], size=1, outfile=output_dir+'family_corr/'+family+'_corr_DScore_vs_DAUC_all.png', show=False)
        print(r)
        print(p)
    
    familiesLow = families[:-3]
    #familiesLow.remove('Vobfus')
    #familiesLow.remove('Zbot')
    
    r,p = compute_correlation(runs0, runsM, num_runs, score_iters, families, size=1, outfile=output_dir+'corr_DScore_vs_DAUC_all.png', show=False)
    print(r)
    print(p)
    print()
    
    r,p = compute_correlation(runs0, runsM, num_runs, score_iters, familiesLow, size=1, outfile=output_dir+'corr_DScore_vs_DAUC_no99.png', show=False)
    print(r)
    print(p)
    print()
    
    r,p = compute_correlation(runs0, runsM, num_runs, [5,10,15,20], familiesLow, size=1, outfile=None, show=False)
    print(r)
    print(p)
    print()

    '''for itr in score_iters:
        print(itr)
        r,p = compute_correlation(runs0, runsM, num_runs, [itr], families[:-2])
        print(r)
        print(p)
    print()
        
    for family in families:
        print(family)
        r,p = compute_correlation(runs0, runsM, num_runs, score_iters, [family])
        print(r)
        print(p)
    print()
    '''
    
    #correlation between mean_score_change at an iter(all families, all runs) and mean_auc_change at that iter(all families,all runs)
    dscores = []
    dAUC = []
    for itr in score_iters:
        dscore = 0
        dauc = 0
        for family in families:
            for i in range(num_runs):
                run0 = runs0[i]
                runM = runsM[i]
                score0 = run0.family_train_histories[family].max()[itr-1]
                scoreM = runM.family_train_histories[family].max()[itr-1]
                dscore += scoreM - score0
                dauc += runM.aucs[itr][family] - run0.aucs[itr][family]
        dscore /= len(families) * num_runs
        dauc /= len(families) * num_runs
        dscores.append(dscore)
        dAUC.append(dauc)
    r,p = pearsonr(dscores, dAUC)
    print(r)
    print(p)
    plt.scatter(dscores, dAUC)
    plt.xlabel("$\Delta$ Mean Score")
    plt.ylabel("$\Delta$ Mean AUC")    
    plt.savefig(output_dir+'corr_IterMeanDScore_vs_IterMeanDAUC.png', dpi=200, bbox_inches='tight')
    #plt.show()
    


    return
    

   
        



if __name__ == "__main__":
    plotMultiIters()
